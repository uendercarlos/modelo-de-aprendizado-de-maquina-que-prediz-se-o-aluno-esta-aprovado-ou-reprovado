{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Uninassau_minicurso.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uendercarlos/sistema-que-diz-se-o-aluno-esta-aprovado-ou-reprovado/blob/main/Uninassau_minicurso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g0lyqVymY08"
      },
      "source": [
        "## ** BÁSICO DE PYTHON **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH9sYNLpmY0_"
      },
      "source": [
        "Falar sobre a linguagem..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15GsqY5aG1EU"
      },
      "source": [
        "### Variavel\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBJU4YBtC3Zw"
      },
      "source": [
        "nome = \"uender\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiitJevNC9E3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b2a2d53-10a9-4ca7-c6a2-ecbca46e4654"
      },
      "source": [
        "nome"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'uender'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWwZULANE_cm"
      },
      "source": [
        "idade = 31"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nakrhTfFHf6N"
      },
      "source": [
        "### Função\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg-jLNnpFFyU"
      },
      "source": [
        "def soma(idade):\n",
        "  print(\"estamos dentro da função\")\n",
        "  return idade + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuxoUwgIGDVs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "972370d9-f5b2-49fe-a656-1025a212156a"
      },
      "source": [
        "soma(31)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "estamos dentro da função\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzuG-pVrILDj"
      },
      "source": [
        "### Lista ou array\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKlXi06YGP9f"
      },
      "source": [
        "produto1 = \"celular\"\n",
        "produto2 = \"capinha\"\n",
        "produto3 = \"pelicula\"\n",
        "\n",
        "produtos = [\"celular\", \"capinha\", \"pelicula\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GipPMv3JYc0"
      },
      "source": [
        "### Laço de repetição \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYiwRVUmI-Z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4ee722-d3dd-4f40-a184-1ab6fe07ea9a"
      },
      "source": [
        "for produto in produtos:\n",
        "  print(produto)\n",
        "  print(\"....\")\n",
        "print(\"estamos fora do laço\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "celular\n",
            "....\n",
            "capinha\n",
            "....\n",
            "pelicula\n",
            "....\n",
            "estamos fora do laço\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO-SYdcjKDG6"
      },
      "source": [
        "### Dicionário ou objeto\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp9ICwPeKVpf"
      },
      "source": [
        "dados_produtos = {\"nome\":\"celular\", \"marca\": \"sansung\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AoaLbw2ML9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d811de53-1dc9-48b6-9ac2-fc9e07c08351"
      },
      "source": [
        "dados_produtos [\"nome\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'celular'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Vu5StSgEmY04"
      },
      "source": [
        "# ** AVANÇADO DE PYTHON **\n",
        "\n",
        "## MACHINE LEARNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3FEgtyiLXU4"
      },
      "source": [
        "Em projetos de ciência de dados visando a construção de modelos de *machine learning*, ou aprendizado estatístico, é muito incomum que os dados iniciais estejam já no formato ideal para a construção de modelos. São necessários vários passos intermediários de pré-processamento de dados, como por exemplo a codificação de variáveis categóricas, normalização de variáveis numéricas, tratamento de dados faltantes, etc. A biblioteca **scikit-learn** -- uma das mais populares bibliotecas de código-aberto para *machine learning* no mundo -- possui diversas funções já integradas para a realização das transformações de dados mais utilizadas. Entretanto, em um fluxo comum de um modelo de aprendizado de máquina, é necessária a aplicação dessas transformações pelo menos duas vezes: a primeira vez para \"treinar\" o modelo, e depois novamente quando novos dados forem enviados como entrada para serem classificados por este modelo. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQWy4WvnmY1B"
      },
      "source": [
        "### Trabalhando com Pipelines do scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WadYn2bbmY1C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "5738d87a-58d2-4b2c-e014-c0b3d053161d"
      },
      "source": [
        "# Primeiro, realizamos a instalação do scikit-learn versão 0.20.3 e do xgboost versão 0.71 no Kernel deste notebook\n",
        "!pip install scikit-learn==0.20.3 --upgrade\n",
        "!pip install xgboost==0.71 --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==0.20.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/82/c0de5839d613b82bddd088599ac0bbfbbbcbd8ca470680658352d2c435bd/scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.3) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.3) (1.18.5)\n",
            "Installing collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.20.3\n",
            "Collecting xgboost==0.71\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/c4/57e246bc99e45c048f9805f2773e7369f0d30896d19fa089fa1794c7b246/xgboost-0.71.tar.gz (494kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost==0.71) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost==0.71) (1.4.1)\n",
            "Building wheels for collected packages: xgboost\n",
            "  Building wheel for xgboost (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xgboost: filename=xgboost-0.71-cp36-cp36m-linux_x86_64.whl size=1957172 sha256=efb9062f1b5fe845c3d6289fcb5ae80b506117417074f4a839f91bcd1cb06a09\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/6d/1d/0bc23240225fe411315d8abb5d4521b9ff002493ff77515ccc\n",
            "Successfully built xgboost\n",
            "Installing collected packages: xgboost\n",
            "  Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-0.71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL2__uTtmY1I"
      },
      "source": [
        "# Em seguida iremos importar diversas bibliotecas que serão utilizadas:\n",
        "\n",
        "# Pacote para trabalhar com JSON\n",
        "import json\n",
        "\n",
        "# Pacote para realizar requisições HTTP\n",
        "import requests\n",
        "\n",
        "# Pacote para exploração e análise de dados\n",
        "import pandas as pd\n",
        "\n",
        "# Pacote com métodos numéricos e representações matriciais\n",
        "import numpy as np\n",
        "\n",
        "# Pacote para construção de modelo baseado na técnica Gradient Boosting\n",
        "import xgboost as xgb\n",
        "\n",
        "# Pacotes do scikit-learn para pré-processamento de dados\n",
        "# \"SimpleImputer\" é uma transformação para preencher valores faltantes em conjuntos de dados\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Pacotes do scikit-learn para treinamento de modelos e construção de pipelines\n",
        "# Método para separação de conjunto de dados em amostras de treino e teste\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Método para criação de modelos baseados em árvores de decisão\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Classe para a criação de uma pipeline de machine-learning\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Pacotes do scikit-learn para avaliação de modelos\n",
        "# Métodos para validação cruzada do modelo criado\n",
        "from sklearn.model_selection import KFold, cross_validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEOu6PMymY1Q"
      },
      "source": [
        "df_data_1 = pd.read_csv(\"dataset_desafio_2.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2zV3gIcmY1Y"
      },
      "source": [
        "### Explorando os dados fornecidos\n",
        "\n",
        "Podemos continuar a exploração dos dados fornecidos com a função ``info()``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol2L23-GmY1Z"
      },
      "source": [
        "df.head(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyzEyR_KmY1g"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtoX9qrBs0nc"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "benPE7vY-nIx"
      },
      "source": [
        "df_data_1.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADHBcdhy-04V"
      },
      "source": [
        "# Exibindo os dados ausentes do conjunto de dados antes da primeira transformação (df)\n",
        "print(\"Valores nulos no df_training_dataset antes da transformação DropNA: \\n\\n{}\\n\".format(df_data_1.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU_CKhGu-6c9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQDdCt63mY1n"
      },
      "source": [
        "### Visualizações\n",
        "\n",
        "Para visualizar o dataset fornecido, podemos utilizar as bibliotecas ``matplotlib`` e ``seaborn``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usmc1XOzmY1o"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cv0P3mPmY1t"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n",
        "\n",
        "sns.countplot(ax=axes[0], x='REPROVACOES_DE', data=df_data_1)\n",
        "sns.countplot(ax=axes[1], x='REPROVACOES_EM', data=df_data_1)\n",
        "sns.countplot(ax=axes[2], x='REPROVACOES_MF', data=df_data_1)\n",
        "sns.countplot(ax=axes[3], x='REPROVACOES_GO', data=df_data_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VhsTcltmY1y"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n",
        "\n",
        "sns.distplot(df_data_1['NOTA_DE'], ax=axes[0])\n",
        "sns.distplot(df_data_1['NOTA_EM'], ax=axes[1])\n",
        "sns.distplot(df_data_1['NOTA_MF'], ax=axes[2])\n",
        "sns.distplot(df_data_1['NOTA_GO'].dropna(), ax=axes[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kj-Qcdk_mY13"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n",
        "\n",
        "sns.countplot(ax=axes[0], x='INGLES', data=df_data_1)\n",
        "sns.countplot(ax=axes[1], x='FALTAS', data=df_data_1)\n",
        "sns.countplot(ax=axes[2], x='H_AULA_PRES', data=df_data_1)\n",
        "sns.countplot(ax=axes[3], x='TAREFAS_ONLINE', data=df_data_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQYFQ7HvmY18"
      },
      "source": [
        "fig = plt.plot()\n",
        "sns.countplot(x='PERFIL', data=df_data_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBDntkMkmY2C"
      },
      "source": [
        "### Realizando o pré-processamento dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K83zNmX4mY2D"
      },
      "source": [
        "Para o pré-processamento dos dados serão apresentadas duas transformações básicas neste notebook, demonstrando a construção de uma Pipeline com um modelo funcional. Esta Pipeline funcional fornecida deverá ser melhorada pelo participante para que o modelo final alcance a maior acurácia possível, garantindo uma pontuação maior no desafio. Essa melhoria pode ser feita apenas no pré-processamento dos dados, na escolha de um algoritmo para treinamento de modelo diferente, ou até mesmo na alteração do *framework* usado (entretanto só será fornecido um exemplo pronto de integração do Watson Machine Learning com o *scikit-learn*).\n",
        "\n",
        "A primeira transformação (passo na nossa Pipeline) será a exclusão da coluna \"NOME\" do nosso dataset, que além de não ser uma variável numérica, também não é uma variável relacionada ao desempenho dos estudantes nas disciplinas. Existem funções prontas no scikit-learn para a realização dessa transformação, entretanto nosso exemplo irá demonstrar como criar uma transformação personalizada do zero no scikit-learn. Se desejado, o participante poderá utilizar esse exemplo para criar outras transformações e adicioná-las à Pipeline final :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1boLMq695sG"
      },
      "source": [
        "### Removendo todas as linhas que possuem algum valor nulos em determinadas colunas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GokeFVEb95sH"
      },
      "source": [
        "Usando o método Pandas **DataFrame.dropna()** você pode remover todas as linhas nulas do dataset.\n",
        "\n",
        "Docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25lrpPz-_Xvi"
      },
      "source": [
        "# Exibindo os dados ausentes do conjunto de dados antes da primeira transformação (df)\n",
        "print(\"Valores nulos no df_training_dataset antes da transformação DropNA: \\n\\n{}\\n\".format(df_data_1.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV7voZGC_eQ2"
      },
      "source": [
        "# Aplicando a função para deletar todas as linhas com valor NaN na coluna ``certificados'' e ``total_modulos'':\n",
        "df_data_1 = df_data_1.dropna(axis='index', how='any', subset=['NOTA_GO', 'INGLES'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGpCfHiL_gYX"
      },
      "source": [
        "# Exibindo os dados ausentes do conjunto de dados após a primeira transformação (df)\n",
        "print(\"Valores nulos no df_training_dataset após a transformação DropNA: \\n\\n{}\\n\".format(df_data_1.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji3BSP6b95sc"
      },
      "source": [
        "Transformação 1:\n",
        "### Processando valores NaN com o SimpleImputer do sklearn\n",
        "\n",
        "Para os valores NaN, usaremos a substituição pela constante 0 como **exemplo**.\n",
        "\n",
        "Você pode escolher a estratégia que achar melhor para tratar os valores nulos :)\n",
        "\n",
        "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html?highlight=simpleimputer#sklearn.impute.SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbA0wsMZmY2n"
      },
      "source": [
        "# Visualizando os dados faltantes do dataset após a primeira transformação (df_data_2)\n",
        "print(\"Valores nulos antes da transformação SimpleImputer: \\n\\n{}\\n\".format(df_data_1.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIzAwrZDmY2j"
      },
      "source": [
        "# Criação de um objeto ``SimpleImputer``\n",
        "si = SimpleImputer(\n",
        "    missing_values=np.nan,  # os valores faltantes são do tipo ``np.nan`` (padrão Pandas)\n",
        "    strategy='constant',  # a estratégia escolhida é a alteração do valor faltante por uma constante\n",
        "    fill_value=0,  # a constante que será usada para preenchimento dos valores faltantes é um int64=0.\n",
        "    verbose=0,\n",
        "    copy=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlOgC9Z1mY2r"
      },
      "source": [
        "# Aplicamos o SimpleImputer ``si`` ao conjunto de dados df_data_2 (resultado da primeira transformação)\n",
        "si.fit(X=df_data_1)\n",
        "\n",
        "# Reconstrução de um novo DataFrame Pandas com o conjunto imputado (df_data_3)\n",
        "df_data_3 = pd.DataFrame.from_records(\n",
        "    data=si.transform(\n",
        "        X=df_data_1\n",
        "    ),  # o resultado SimpleImputer.transform(<<pandas dataframe>>) é lista de listas\n",
        "    columns=df_data_1.columns  # as colunas originais devem ser conservadas nessa transformação\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZFoxfQymY2y"
      },
      "source": [
        "# Visualizando os dados faltantes do dataset após a segunda transformação (SimpleImputer) (df_data_3)\n",
        "print(\"Valores nulos no dataset após a transformação SimpleImputer: \\n\\n{}\\n\".format(df_data_3.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCWj6-bmXmPa"
      },
      "source": [
        "df_data_2.head(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb_NJam7mY22"
      },
      "source": [
        "Nota-se que não temos mais nenhum valor faltante no nosso conjunto de dados :)\n",
        "\n",
        "Vale salientar que nem sempre a alteração dos valores faltantes por 0 é a melhor estratégia. O participante é incentivado a estudar e implementar estratégias diferentes de tratamento dos valores faltantes para aprimorar seu modelo e melhorar sua pontuação final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJM_SoohmY2E"
      },
      "source": [
        "## Transformação 2: \n",
        "excluindo colunas do dataset\n",
        "\n",
        "Para a criação de uma transformação de dados personalizada no scikit-learn, é necessária basicamente a criação de uma classe com os métodos ``transform`` e ``fit``. No método transform será executada a lógica da nossa transformação.\n",
        "\n",
        "Na próxima célula é apresentado o código completo de uma transformação ``DropColumns`` para a remoção de colunas de um DataFrame pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byd3ubL_mY2F"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# All sklearn Transforms must have the `transform` and `fit` methods\n",
        "class DropColumns(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        # Primeiro realizamos a cópia do dataframe 'X' de entrada\n",
        "        data = X.copy()\n",
        "        # Retornamos um novo dataframe sem as colunas indesejadas\n",
        "        return data.drop(labels=self.columns, axis='columns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOFFdGZumY2L"
      },
      "source": [
        "Para aplicar essa transformação em um DataFrame pandas, basta instanciar um objeto *DropColumns* e chamar o método transform()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16BykmGemY2L"
      },
      "source": [
        "# Instanciando uma transformação DropColumns\n",
        "rm_columns = DropColumns(\n",
        "    columns=[\"NOME\"]  # Essa transformação recebe como parâmetro uma lista com os nomes das colunas indesejadas\n",
        ")\n",
        "\n",
        "print(rm_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PxsmiMXmY2T"
      },
      "source": [
        "# Visualizando as colunas do dataset original\n",
        "print(\"Colunas do dataset original: \\n\")\n",
        "print(df_data_1.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKCZafOvmY2X"
      },
      "source": [
        "# Aplicando a transformação ``DropColumns`` ao conjunto de dados base\n",
        "rm_columns.fit(X=df_data_3)\n",
        "\n",
        "# Reconstruindo um DataFrame Pandas com o resultado da transformação\n",
        "df_data_2 = pd.DataFrame.from_records(\n",
        "    data=rm_columns.transform(\n",
        "        X=df_data_3\n",
        "    ),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psTECYY7mY2c"
      },
      "source": [
        "# Visualizando as colunas do dataset transformado\n",
        "print(\"Colunas do dataset após a transformação ``DropColumns``: \\n\")\n",
        "print(df_data_2.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMG2E00IQhND"
      },
      "source": [
        "df_data_2.head(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvQuNq3lmY2h"
      },
      "source": [
        "Nota-se que a coluna \"NOME\" foi removida e nosso dataset agora poossui apenas 17 colunas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEiq4DdYmY23"
      },
      "source": [
        "## Treinando um modelo de classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFMxvbplmY24"
      },
      "source": [
        "Finalizado o pré-processamento, já temos o conjunto de dados no formato necessário para o treinamento do nosso modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmN7Dqe6mY25"
      },
      "source": [
        "df_data_3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9G9yog1mY29"
      },
      "source": [
        "No exemplo fornecido, iremos utilizar todas as colunas, exceto a coluna **LABELS** como *features* (variáveis de entrada).\n",
        "\n",
        "A variável **LABELS** será a variável-alvo do modelo, conforme descrito no enunciado do desafio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP5-6BRJmY2-"
      },
      "source": [
        "## Definindo as features do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4yD6lMYmY2_"
      },
      "source": [
        "# Definição das colunas que serão features (nota-se que a coluna NOME não está presente)\n",
        "features = [\n",
        "    \"MATRICULA\", 'REPROVACOES_DE', 'REPROVACOES_EM', \"REPROVACOES_MF\", \"REPROVACOES_GO\",\n",
        "    \"NOTA_DE\", \"NOTA_EM\", \"NOTA_MF\", \"NOTA_GO\",\n",
        "    \"INGLES\", \"H_AULA_PRES\", \"TAREFAS_ONLINE\", \"FALTAS\", \n",
        "]\n",
        "\n",
        "# Definição da variável-alvo\n",
        "target = [\"PERFIL\"]\n",
        "\n",
        "# Preparação dos argumentos para os métodos da biblioteca ``scikit-learn``\n",
        "X = df_data_3[features]\n",
        "y = df_data_3[target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0YosyFhmY3I"
      },
      "source": [
        "O conjunto de entrada (X):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbVlNVHPmY3J"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS_txtY-mY3O"
      },
      "source": [
        "As variáveis-alvo correspondentes (y):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4p-1WeEmY3P"
      },
      "source": [
        "y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbg9zC4mmY3X"
      },
      "source": [
        "## Separando o dataset em um conjunto de treino e um conjunto de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7pDea5omY3Y"
      },
      "source": [
        "Iremos separar o dataset fornecido em dois grupos: um para treinar nosso modelo, e outro para testarmos o resultado através de um teste cego. A separação do dataset pode ser feita facilmente com o método *train_test_split()* do scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "virc4JjymY3Y"
      },
      "source": [
        "# Separação dos dados em um conjunto de treino e um conjunto de teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2D0EwccmY3d"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdh8j2_mY3e"
      },
      "source": [
        "## Criando um modelo baseado em árvores de decisão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhlIH7cWmY3f"
      },
      "source": [
        "No exemplo fornecido iremos criar um classificador baseado em **árvores de decisão**.\n",
        "\n",
        "Material teórico sobre árvores de decisão na documentação oficial do scikit-learn: https://scikit-learn.org/stable/modules/tree.html\n",
        "\n",
        "O primeiro passo é basicamente instanciar um objeto *DecisionTreeClassifier()* da biblioteca scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLdqolrXmY3i"
      },
      "source": [
        "# Criação de uma árvore de decisão com a biblioteca ``scikit-learn``:\n",
        "decision_tree = DecisionTreeClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BTjY6oDmY3m"
      },
      "source": [
        "## Testando o classificador baseado em árvore de decisão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG0-6xNWmY3n"
      },
      "source": [
        "# Treino do modelo (é chamado o método *fit()* com os conjuntos de treino)\n",
        "decision_tree.fit(\n",
        "    X_train,\n",
        "    y_train\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddxsP3_kmY3s"
      },
      "source": [
        "## Execução de predições e avaliação da árvore de decisão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcDnczDAmY3t"
      },
      "source": [
        "# Realização de teste cego no modelo criado\n",
        "y_pred = decision_tree.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbn-k0XCmY3y"
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YScRfOQ3mY33"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KXleV40mY38"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Acurácia alcançada pela árvore de decisão\n",
        "pred), 2rint(\"Acurácia: {}%\".format(100*round(accuracy_score(y_test, y_p)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFkpg2E95ul"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnUmrudhALnM"
      },
      "source": [
        "from google.colab import files\n",
        "df.to_csv('filename.csv') \n",
        "files.download('filename.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ygOATtMmY4H"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-LkA10_mY4I"
      },
      "source": [
        "Neste notebook foi demonstrado como trabalhar com transformações e modelos com a biblioteca scikit-learn. É recomendado que o participante realize seus experimentos editando o código fornecido aqui até que um modelo com acurácia elevada seja alcançado.\n",
        "\n",
        "Quando você estiver satisfeito com seu modelo, pode passar para a segunda etapa do desafio -- encapsular seu modelo como uma API REST pronta para uso em uma ferramenta Machine Learning!\n",
        "\n"
      ]
    }
  ]
}